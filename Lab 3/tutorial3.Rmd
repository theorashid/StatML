---
title: "Machine Learning -- Tutorial3"
author: "Sarah Filippi"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Principal Component Analysis

## US arrests dataset

Consider the US arrests dataset. This dataset contains statistics in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas. Please look at the description of the dataset.

```{r, echo=T}
?USArrests
```

We start by looking at the data with a pairplot.

```{r, echo=T, fig=T}
pairs(USArrests)
```

We then run the Principal Component Analysis. We will first scale the data.

```{r, echo=T}
df <- scale(USArrests)
usarrests.pca <- princomp(df)
summary(usarrests.pca)
```

Note that correlation matrix can be used instead of the covariance which is a common practice when variables have very different ranges and scales. Another common practice is to normalise the data by ensuring that every variable has same mean and variance before applying PCA. 

Suggestion: Try to run the previous command with correlation or covariance. Also try to re-scale the data prior to performing PCA.

The function returns 
* the square root of the eigenvalues of the correlation matrix.
* the matrix containing the eigen vectors (loadings)

You can compare the output of the PCA to the eigen vectors and eigenvalues of the matrix S defined in the lecture notes.

```{r}
print("eigenvalues of X^T X")
eval <- (eigen(t(df) %*% df)$value)
sqrt(eval)
print("sdev by princomp")
usarrests.pca$sdev
print("eigenvectors of X^T X ")
evec <- eigen(t(df) %*% df)$vector
evec
print("loadings by princomp")
usarrests.pca$loadings
```


How much variance is explained by each component?

```{r, echo=T, fig=T}
plot(usarrests.pca)
```

We then produce a pair plot to observe the representation of the observation on the basis spanned by the principal components. 

```{r, fig=T}
y <- df%*%evec
pairs(y)
```

The folowing biplot shows the observations in the space of the two first principal components. It also allows us to visualise the linear combination of each component. 

```{r, fig=T}
biplot(usarrests.pca, scale =1)
```

Here we can see that the first principal component is a linear combination of the four variables whereas the second principal component is mainly influenced by urban population. This can also be seen by directly looking at the eigenvectors.

## Iris dataset

Perform a Principal Component Analysis on the Iris dataset.

## MNIST dataset

The MNIST (Modified National Institute of Standards and Technology) data is a database
of handwritten digits.

```{r}
mnist <- read.csv("http://wwwf.imperial.ac.uk/~sfilippi/Data/mnist_train.csv", header = F)
```

Each handwritten digit is stored in a 28 Ã— 28 pixels grayscale image. The grayscale
values (0 to 255) for the 784 pixels are stored as row vectors in the dataset. The first columns is the lable of the image, i.e a digit between 0 and 9. Below is a  function to graphically display a digit based on its vector of grayscale values.

```{r}
visualise = function(vec, ...){ # function for graphically displaying a digit
image(matrix(as.numeric(vec),nrow=28)[,28:1], col=gray((255:0)/255), ...)
}
old_par <- par(mfrow=c(2,2))
for (i in 1:4) visualise(mnist[i,-1])
par(old_par)
```
Consider a subsample of the dataset containing the first 1000 observations

1. Compute the covariance matrix S. Remember to first center the data.

```{r, echo=T}
m<-colMeans(mnist[1:1000,-1])
mdata <- scale(mnist[1:1000,-1], center=T, scale=F )
S <- t(mdata)%*% mdata
```

2. Compute the eigenvalues/eigenvectors of S. Plot the percentage of variance explained by each principal component

```{r}
eval <- eigen(S)$value
evec <- eigen(S)$vector
plot(eval/sum(eval)*100, ylab="% of variance explained", xlab="principal components")
```

3. Consider the first image x in the digits dataset, given by the first row of the mnist data matrix. Make a plot showing the distance in 2-norm between x and its reconstruction for different numbers of compression dimensions.

```{r, echo=T}
x <- mdata[1,]
error<- sapply(seq(1,780,by=10), function(i) sum((x-evec[,1:i]%*%t(evec[,1:i])%*%x)^2))
plot(seq(1,780,by=10), error, xlab="nb of dimensions")
```

4. Plot the original image x as well as the reconstructed image when using 10, 20, 50, 100 and 200 compression dimensions.
```{r, echo=T}

old_par <- par(mfrow=c(3,2))

visualise(x+m)
visualise(evec[,1:10]%*%t(evec[,1:10])%*%x+m)
visualise(evec[,1:20]%*%t(evec[,1:20])%*%x+m)
visualise(evec[,1:50]%*%t(evec[,1:50])%*%x+m)
visualise(evec[,1:100]%*%t(evec[,1:100])%*%x+m)
visualise(evec[,1:200]%*%t(evec[,1:200])%*%x+m)
par(old_par)
```
# Clustering

## K-means 
Consider the US arrest dataset. Start by normalising the data so that each variable has a mean 0 and a variance 1.

```{r, echo=T}
df <- scale(USArrests)
```

Run the K-mean algorithm with K=4.

```{r, echo=T}
set.seed(123)
km.res <- kmeans(df, centers=4)
km.res
```

Run the algorithm with different seeds and observe the variability in the results.

Visualise the clusters by plotting the projected data in the plane spanned by the two first principal components and colouring the data by their cluster allocation.

```{r, echo=T}
library(ggfortify)
#autoplot(princomp(df), colour=km.res$cluster)
autoplot(km.res, data = df)
```


## Hierarchical clustering

To perform hierarchical clustering, we first need to compute the distance between each observation. Let's use the euclidian distance and compute a distance matrix as follows:

```{r, echo=T}
dist_data<-dist(df, method = 'euclidean')
```

We can then perform hierarchical clustering using the complete linkage criteria and plot the resulting dendogram.

```{r, echo=T}
hdata<-hclust(dist_data)
plot(hdata)
abline(h=3.75, lty=2) 
```

We can obtain the cluster allocation for each observation when cutting the dendogram at a given height using the following command:

```{r, echo=T}
alloc <- cutree(hdata, h=3.75)
```

Visualise the clusters by plotting the projection of the data in the plane spanned by the two first principal components and colouring the data by their cluster allocation.

Perform hierarchical clustering with different distances and different linkage criteria. 

